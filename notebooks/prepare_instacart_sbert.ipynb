{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Instacart data preparation for two-tower SBERT\n",
    "\n",
    "Step-by-step investigation of each function output. Uses a small subset by default (`max_train_orders=500`) for fast iteration."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup: paths and config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DATA_DIR: /Users/chen_bowen/AI & ML/Projects/Instacart_Personalization/data\n",
      "OUTPUT_DIR: /Users/chen_bowen/AI & ML/Projects/Instacart_Personalization/processed\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "from collections import defaultdict\n",
    "\n",
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "\n",
    "# Project root (parent of notebooks/)\n",
    "PROJECT_ROOT = Path.cwd().parent if Path.cwd().name == \"notebooks\" else Path.cwd()\n",
    "DATA_DIR = PROJECT_ROOT / \"data\"\n",
    "OUTPUT_DIR = PROJECT_ROOT / \"processed\"\n",
    "CHUNK_SIZE = 500_000\n",
    "\n",
    "# Config (change for quick vs full run)\n",
    "MAX_TRAIN_ORDERS = 500\n",
    "MAX_PRIOR_ORDERS = 20  # keep 20 prior orders, matching script default\n",
    "MAX_PRODUCT_NAMES = 80\n",
    "EVAL_FRAC = 0.1\n",
    "SAMPLE_FRAC = None\n",
    "SEED = 42\n",
    "\n",
    "print(\"DATA_DIR:\", DATA_DIR)\n",
    "print(\"OUTPUT_DIR:\", OUTPUT_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. `load_product_text_map`\n",
    "\n",
    "Builds `product_id -> \"Product: {name}. Aisle: {aisle}. Department: {department}.\"`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_product_text_map(products_path, aisles_path, departments_path):\n",
    "    \"\"\"\n",
    "    Build a mapping from product_id to a single text string for the item tower.\n",
    "\n",
    "    Joins products with aisle and department names, then formats each product as\n",
    "    \"Product: {name}. Aisle: {aisle}. Department: {department}.\" for use as\n",
    "    the \"positive\" (item) side of (anchor, positive) pairs.\n",
    "\n",
    "    Args:\n",
    "        products_path: Path to products.csv (product_id, product_name, aisle_id, department_id).\n",
    "        aisles_path: Path to aisles.csv (aisle_id, aisle).\n",
    "        departments_path: Path to departments.csv (department_id, department).\n",
    "\n",
    "    Returns:\n",
    "        Dict mapping product_id (int) to the formatted product text (str).\n",
    "    \"\"\"\n",
    "    products = pd.read_csv(products_path)\n",
    "    aisles = pd.read_csv(aisles_path)\n",
    "    departments = pd.read_csv(departments_path)\n",
    "    products = products.merge(aisles, on=\"aisle_id\").merge(departments, on=\"department_id\")\n",
    "    products[\"text\"] = (\n",
    "        \"Product: \" + products[\"product_name\"].astype(str)\n",
    "        + \". Aisle: \" + products[\"aisle\"].astype(str)\n",
    "        + \". Department: \" + products[\"department\"].astype(str) + \".\"\n",
    "    )\n",
    "    return dict(zip(products[\"product_id\"], products[\"text\"]))\n",
    "\n",
    "product_text_map = load_product_text_map(\n",
    "    DATA_DIR / \"products.csv\",\n",
    "    DATA_DIR / \"aisles.csv\",\n",
    "    DATA_DIR / \"departments.csv\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Len: 49688\n",
      "\n",
      "Sample (first 5 product_ids):\n",
      "  1: Product: Chocolate Sandwich Cookies. Aisle: cookies cakes. Department: snacks....\n",
      "  2: Product: All-Seasons Salt. Aisle: spices seasonings. Department: pantry....\n",
      "  3: Product: Robust Golden Unsweetened Oolong Tea. Aisle: tea. Department: beverages...\n",
      "  4: Product: Smart Ones Classic Favorites Mini Rigatoni With Vodka Cream Sauce. Aisl...\n",
      "  5: Product: Green Chile Anytime Sauce. Aisle: marinades meat preparation. Departmen...\n"
     ]
    }
   ],
   "source": [
    "# Inspect output\n",
    "print(\"Len:\", len(product_text_map))\n",
    "print(\"\\nSample (first 5 product_ids):\")\n",
    "for pid, text in list(product_text_map.items())[:5]:\n",
    "    print(f\"  {pid}: {text[:80]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. `load_orders`\n",
    "\n",
    "Returns `(target_orders, history_orders)` DataFrames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_orders(orders_path):\n",
    "    \"\"\"\n",
    "    Load orders.csv and split into train vs prior by eval_set.\n",
    "\n",
    "    Train orders are the \"next\" order we predict for each user; prior orders\n",
    "    are that user's history used only to build context (no leakage).\n",
    "\n",
    "    Args:\n",
    "        orders_path: Path to orders.csv (order_id, user_id, eval_set, order_number, etc.).\n",
    "\n",
    "    Returns:\n",
    "        Tuple of (target_orders, history_orders), each a DataFrame with relevant columns.\n",
    "    \"\"\"\n",
    "    orders = pd.read_csv(orders_path)\n",
    "    if orders[\"order_hour_of_day\"].dtype == object:\n",
    "        orders[\"order_hour_of_day\"] = orders[\"order_hour_of_day\"].astype(str).str.zfill(2)\n",
    "    target_orders = orders[orders[\"eval_set\"] == \"train\"][\n",
    "        [\"order_id\", \"user_id\", \"order_number\", \"order_dow\", \"order_hour_of_day\", \"days_since_prior_order\"]\n",
    "    ].copy()\n",
    "    # history orders now keep dow / hour / days_since_prior_order so we can encode per-order time features\n",
    "    history_orders = orders[orders[\"eval_set\"] == \"prior\"][\n",
    "        [\"order_id\", \"user_id\", \"order_number\", \"order_dow\", \"order_hour_of_day\", \"days_since_prior_order\"]\n",
    "    ].copy()\n",
    "    return target_orders, history_orders\n",
    "\n",
    "target_orders_full, history_orders_full = load_orders(DATA_DIR / \"orders.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "target_orders shape: (131209, 6)\n",
      "history_orders shape: (3214874, 6)\n",
      "\n",
      "target_orders head:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>order_id</th>\n",
       "      <th>user_id</th>\n",
       "      <th>order_number</th>\n",
       "      <th>order_dow</th>\n",
       "      <th>order_hour_of_day</th>\n",
       "      <th>days_since_prior_order</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1187899</td>\n",
       "      <td>1</td>\n",
       "      <td>11</td>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "      <td>14.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>1492625</td>\n",
       "      <td>2</td>\n",
       "      <td>15</td>\n",
       "      <td>1</td>\n",
       "      <td>11</td>\n",
       "      <td>30.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>2196797</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>525192</td>\n",
       "      <td>7</td>\n",
       "      <td>21</td>\n",
       "      <td>2</td>\n",
       "      <td>11</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>880375</td>\n",
       "      <td>8</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>14</td>\n",
       "      <td>10.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>1094988</td>\n",
       "      <td>9</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>10</td>\n",
       "      <td>30.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>1822501</td>\n",
       "      <td>10</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>19</td>\n",
       "      <td>30.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>115</th>\n",
       "      <td>1827621</td>\n",
       "      <td>13</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>21</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>129</th>\n",
       "      <td>2316178</td>\n",
       "      <td>14</td>\n",
       "      <td>14</td>\n",
       "      <td>2</td>\n",
       "      <td>19</td>\n",
       "      <td>11.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200</th>\n",
       "      <td>2180313</td>\n",
       "      <td>17</td>\n",
       "      <td>41</td>\n",
       "      <td>3</td>\n",
       "      <td>10</td>\n",
       "      <td>30.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     order_id  user_id  order_number  order_dow  order_hour_of_day  \\\n",
       "10    1187899        1            11          4                  8   \n",
       "25    1492625        2            15          1                 11   \n",
       "49    2196797        5             5          0                 11   \n",
       "74     525192        7            21          2                 11   \n",
       "78     880375        8             4          1                 14   \n",
       "82    1094988        9             4          6                 10   \n",
       "88    1822501       10             6          0                 19   \n",
       "115   1827621       13            13          0                 21   \n",
       "129   2316178       14            14          2                 19   \n",
       "200   2180313       17            41          3                 10   \n",
       "\n",
       "     days_since_prior_order  \n",
       "10                     14.0  \n",
       "25                     30.0  \n",
       "49                      6.0  \n",
       "74                      6.0  \n",
       "78                     10.0  \n",
       "82                     30.0  \n",
       "88                     30.0  \n",
       "115                     8.0  \n",
       "129                    11.0  \n",
       "200                    30.0  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "history_orders head:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>order_id</th>\n",
       "      <th>user_id</th>\n",
       "      <th>order_number</th>\n",
       "      <th>order_dow</th>\n",
       "      <th>order_hour_of_day</th>\n",
       "      <th>days_since_prior_order</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2539329</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2398795</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>15.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>473747</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>12</td>\n",
       "      <td>21.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2254736</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>7</td>\n",
       "      <td>29.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>431534</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>15</td>\n",
       "      <td>28.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>3367565</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "      <td>19.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>550135</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>20.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>3108588</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>14</td>\n",
       "      <td>14.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2295261</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>16</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2550362</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "      <td>30.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   order_id  user_id  order_number  order_dow  order_hour_of_day  \\\n",
       "0   2539329        1             1          2                  8   \n",
       "1   2398795        1             2          3                  7   \n",
       "2    473747        1             3          3                 12   \n",
       "3   2254736        1             4          4                  7   \n",
       "4    431534        1             5          4                 15   \n",
       "5   3367565        1             6          2                  7   \n",
       "6    550135        1             7          1                  9   \n",
       "7   3108588        1             8          1                 14   \n",
       "8   2295261        1             9          1                 16   \n",
       "9   2550362        1            10          4                  8   \n",
       "\n",
       "   days_since_prior_order  \n",
       "0                     NaN  \n",
       "1                    15.0  \n",
       "2                    21.0  \n",
       "3                    29.0  \n",
       "4                    28.0  \n",
       "5                    19.0  \n",
       "6                    20.0  \n",
       "7                    14.0  \n",
       "8                     0.0  \n",
       "9                    30.0  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"target_orders shape:\", target_orders_full.shape)\n",
    "print(\"history_orders shape:\", history_orders_full.shape)\n",
    "print(\"\\ntarget_orders head:\")\n",
    "display(target_orders_full.head(10))\n",
    "print(\"\\nhistory_orders head:\")\n",
    "display(history_orders_full.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After limiting: target_orders 500 , history_orders 7663 , history_order_ids 7663\n"
     ]
    }
   ],
   "source": [
    "# Limit to subset for fast iteration\n",
    "target_orders = target_orders_full.head(MAX_TRAIN_ORDERS)\n",
    "users_needed = set(target_orders[\"user_id\"].tolist())\n",
    "history_orders = history_orders_full[history_orders_full[\"user_id\"].isin(users_needed)]\n",
    "history_order_ids = set(history_orders[\"order_id\"].tolist())\n",
    "\n",
    "print(\"After limiting: target_orders\", target_orders.shape[0], \", history_orders\", history_orders.shape[0], \", history_order_ids\", len(history_order_ids))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. `build_order_to_products`\n",
    "\n",
    "Builds `order_id -> list of product_id` for prior orders (chunked read)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_order_to_products(order_products_prior_path, history_order_ids, chunk_size=CHUNK_SIZE):\n",
    "    \"\"\"\n",
    "    Build a mapping from each history order_id to the list of product_ids in that order.\n",
    "\n",
    "    Reads order_products__prior.csv in chunks to avoid loading ~32M rows at once.\n",
    "    Only rows whose order_id is in history_order_ids are kept (e.g. orders for users\n",
    "    we care about when using max_train_orders).\n",
    "\n",
    "    Args:\n",
    "        order_products_prior_path: Path to order_products__prior.csv.\n",
    "        history_order_ids: Set of order_ids to include (typically history orders for our users).\n",
    "        chunk_size: Number of rows per chunk when reading the CSV.\n",
    "\n",
    "    Returns:\n",
    "        Dict mapping order_id (int) to list of product_id (int) in that order.\n",
    "    \"\"\"\n",
    "    order_to_products = defaultdict(list)\n",
    "    for chunk in pd.read_csv(order_products_prior_path, chunksize=chunk_size):\n",
    "        chunk = chunk[chunk[\"order_id\"].isin(history_order_ids)]\n",
    "        for order_id, product_id in chunk[[\"order_id\", \"product_id\"]].itertuples(index=False):\n",
    "            order_to_products[order_id].append(product_id)\n",
    "    return dict(order_to_products)\n",
    "\n",
    "order_to_products = build_order_to_products(DATA_DIR / \"order_products__prior.csv\", history_order_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num orders with products: 7663\n",
      "Products per order: min 1 , max 89 , mean 9.778285266866762\n",
      "\n",
      "Sample (first 3 order_id -> product_ids):\n",
      "  order_id 40: 4 products, first 5 ids: [10070, 42450, 33198, 34866]\n",
      "  order_id 1483: 12 products, first 5 ids: [32818, 27582, 12302, 1831, 19204]\n",
      "  order_id 2199: 15 products, first 5 ids: [24852, 17616, 47877, 33313, 651]\n"
     ]
    }
   ],
   "source": [
    "print(\"Num orders with products:\", len(order_to_products))\n",
    "lens = [len(v) for v in order_to_products.values()]\n",
    "print(\"Products per order: min\", min(lens), \", max\", max(lens), \", mean\", sum(lens)/len(lens))\n",
    "print(\"\\nSample (first 3 order_id -> product_ids):\")\n",
    "for oid, pids in list(order_to_products.items())[:3]:\n",
    "    print(f\"  order_id {oid}: {len(pids)} products, first 5 ids: {pids[:5]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. `build_user_context_for_target_orders`\n",
    "\n",
    "For each target order, build user context string from order history only."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_user_context_for_target_orders(\n",
    "    target_orders: pd.DataFrame,\n",
    "    history_orders: pd.DataFrame,\n",
    "    order_to_products: dict[int, list[int]],\n",
    "    product_text_map: dict[int, str],\n",
    "    max_prior_orders: int = 20,\n",
    "    max_product_names: int = 80,\n",
    ") -> dict[int, str]:\n",
    "    \"\"\"\n",
    "    For each target order, build one user-context string using only that user's order history.\n",
    "\n",
    "    Context format encodes per-order time features:\n",
    "      \"Previously ordered: [X days since prior, weekday d, hour h] item1, item2; ...\n",
    "       Next order: weekday D, hour H, days since prior: K.\"\n",
    "\n",
    "    Used as the \"anchor\" (query) side in (anchor, positive) pairs. No leakage: only\n",
    "    history orders with order_number < this target order are used.\n",
    "\n",
    "    Args:\n",
    "        target_orders: DataFrame of orders we predict (order_id, user_id, order_number, order_dow, etc.).\n",
    "        history_orders: DataFrame of past orders used only for context (order_id, user_id, order_number, order_dow, order_hour_of_day, days_since_prior_order).\n",
    "        order_to_products: Mapping from order_id to list of product_ids (for history orders).\n",
    "        product_text_map: Mapping from product_id to full product text (used to get name only).\n",
    "        max_prior_orders: Max number of history orders per user to consider.\n",
    "        max_product_names: Max number of product names to include in the context string.\n",
    "\n",
    "    Returns:\n",
    "        Dict mapping target order_id (int) to the user context string (str).\n",
    "    \"\"\"\n",
    "    # Sort so we can take \"last N\" history orders per user by order_number.\n",
    "    history_orders = history_orders.sort_values([\"user_id\", \"order_number\"])\n",
    "    order_id_to_context: dict[int, str] = {}\n",
    "\n",
    "    for _, row in target_orders.iterrows():\n",
    "        order_id = row[\"order_id\"]\n",
    "        # Get the history orders for the user that happened before the target order.\n",
    "        user_history = history_orders[\n",
    "            (history_orders[\"user_id\"] == row[\"user_id\"]) & (history_orders[\"order_number\"] < row[\"order_number\"])\n",
    "        ].tail(max_prior_orders)\n",
    "\n",
    "        segments: list[str] = []\n",
    "        total_products = 0\n",
    "\n",
    "        # Walk each prior order in chronological order and encode its time features + products.\n",
    "        for _, h in user_history.iterrows():\n",
    "            if total_products >= max_product_names:\n",
    "                break\n",
    "            oid = h[\"order_id\"]\n",
    "            order_products = []\n",
    "            for pid in order_to_products.get(oid, []):\n",
    "                if pid not in product_text_map:\n",
    "                    continue\n",
    "                if total_products >= max_product_names:\n",
    "                    break\n",
    "                name = product_text_map[pid].split(\"Product: \")[1].split(\".\")[0].strip()\n",
    "                order_products.append(name)\n",
    "                total_products += 1\n",
    "\n",
    "            if not order_products:\n",
    "                continue\n",
    "\n",
    "            dow = int(h[\"order_dow\"])\n",
    "            hour = h[\"order_hour_of_day\"]\n",
    "            if pd.isna(h[\"days_since_prior_order\"]):\n",
    "                time_prefix = f\"ordered on weekday {dow} at hour {hour}\"\n",
    "            else:\n",
    "                days_gap = int(h[\"days_since_prior_order\"])\n",
    "                time_prefix = f\"ordered {days_gap} days after previous order on weekday {dow} at hour {hour}\"\n",
    "\n",
    "            seg = f\"[{time_prefix}] \" + \", \".join(order_products)\n",
    "            segments.append(seg)\n",
    "\n",
    "        products_str = \"; \".join(segments) if segments else \"(no prior orders)\"\n",
    "\n",
    "        if pd.isna(row[\"days_since_prior_order\"]):\n",
    "            next_clause = f\"Next order: weekday {row['order_dow']}, hour {row['order_hour_of_day']}.\"\n",
    "        else:\n",
    "            gap = int(row[\"days_since_prior_order\"])\n",
    "            next_clause = (\n",
    "                f\"Next order: {gap} days after previous order on \"\n",
    "                f\"weekday {row['order_dow']} at hour {row['order_hour_of_day']}.\"\n",
    "            )\n",
    "        context = f\"Previously ordered: {products_str}. {next_clause}\"\n",
    "        order_id_to_context[order_id] = context\n",
    "\n",
    "    return order_id_to_context\n",
    "\n",
    "order_id_to_context = build_user_context_for_target_orders(\n",
    "    target_orders, history_orders, order_to_products, product_text_map,\n",
    "    max_prior_orders=MAX_PRIOR_ORDERS, max_product_names=MAX_PRODUCT_NAMES,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num train orders with context: 500\n",
      "\n",
      "Sample contexts (first 2):\n",
      "\n",
      "order_id 1187899.0:\n",
      "Previously ordered: [ordered on weekday 2 at hour 8.0] Soda, Organic Unsweetened Vanilla Almond Milk, Original Beef Jerky, Aged White Cheddar Popcorn, XL Pick-A-Size Paper Towel Rolls; [ordered 15 days after previous order on weekday 3 at hour 7.0] Soda, Pistachios, Original Beef Jerky, Bag of Organ...\n",
      "\n",
      "order_id 1492625.0:\n",
      "Previously ordered: [ordered on weekday 2 at hour 11.0] Chipotle Beef & Pork Realstick, Organic Avocado, Roasted Turkey, Baked Organic Sea Salt Crunchy Pea Snack, Thin Stackers Brown Rice Lightly Salted, Cheddar Bunnies Snack Crackers, Plantain Chips, Organic Just Concord Grape Juice, Uncured Genoa ...\n"
     ]
    }
   ],
   "source": [
    "print(\"Num train orders with context:\", len(order_id_to_context))\n",
    "print(\"\\nSample contexts (first 2):\")\n",
    "for oid, ctx in list(order_id_to_context.items())[:2]:\n",
    "    print(f\"\\norder_id {oid}:\")\n",
    "    print(ctx[:300] + \"...\" if len(ctx) > 300 else ctx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. `build_anchor_positive_pairs`\n",
    "\n",
    "Build (anchor, positive, order_id) from order_products__train."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_anchor_positive_pairs(order_products_train_path, order_id_to_context, product_text_map):\n",
    "    \"\"\"\n",
    "    Build (anchor, positive) training pairs from order_products__train.\n",
    "\n",
    "    Each row in the train order-products table gives one positive pair: the user\n",
    "    context for that order (anchor) and the product text (positive). Also returns\n",
    "    order_id per row so we can split train/eval by order without leakage.\n",
    "\n",
    "    Args:\n",
    "        order_products_train_path: Path to order_products__train.csv (order_id, product_id, ...).\n",
    "        order_id_to_context: Mapping from train order_id to user context string.\n",
    "        product_text_map: Mapping from product_id to product text.\n",
    "\n",
    "    Returns:\n",
    "        Tuple of (anchors, positives, order_ids): parallel lists of same length.\n",
    "    \"\"\"\n",
    "    train_op = pd.read_csv(order_products_train_path)\n",
    "    anchors, positives, order_ids = [], [], []\n",
    "    for _, row in train_op.iterrows():\n",
    "        order_id = row[\"order_id\"]\n",
    "        product_id = row[\"product_id\"]\n",
    "        if order_id not in order_id_to_context or product_id not in product_text_map:\n",
    "            continue\n",
    "        anchors.append(order_id_to_context[order_id])\n",
    "        positives.append(product_text_map[product_id])\n",
    "        order_ids.append(order_id)\n",
    "    return anchors, positives, order_ids\n",
    "\n",
    "anchors, positives, order_ids = build_anchor_positive_pairs(\n",
    "    DATA_DIR / \"order_products__train.csv\",\n",
    "    order_id_to_context,\n",
    "    product_text_map,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total pairs (before split): 5163\n",
      "\n",
      "Sample pair 0:\n",
      "ANCHOR: Previously ordered: [7 days since prior, weekday 4, hour 12.0] Variety Pack Hard Cider, Instant Oatmeal Maple & Brown Sugar; [4 days since prior, weekday 1, hour 8.0] French Vanilla Coffee Creamer, Ha ...\n",
      "\n",
      "POSITIVE: Product: Natural Vanilla Ice Cream. Aisle: ice cream ice. Department: frozen.\n"
     ]
    }
   ],
   "source": [
    "print(\"Total pairs (before split):\", len(anchors))\n",
    "print(\"\\nSample pair 0:\")\n",
    "print(\"ANCHOR:\", anchors[0][:200], \"...\")\n",
    "print(\"\\nPOSITIVE:\", positives[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Train / eval split and final datasets\n",
    "\n",
    "Split by order; optionally sample train."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_order_ids_all = set(order_id_to_context.keys())\n",
    "order_list = sorted(train_order_ids_all)\n",
    "n_eval = max(1, int(len(order_list) * EVAL_FRAC))\n",
    "eval_order_ids = set(order_list[-n_eval:])\n",
    "\n",
    "train_anchors, train_positives = [], []\n",
    "eval_anchors, eval_positives = [], []\n",
    "for a, p, oid in zip(anchors, positives, order_ids):\n",
    "    if oid in eval_order_ids:\n",
    "        eval_anchors.append(a)\n",
    "        eval_positives.append(p)\n",
    "    else:\n",
    "        train_anchors.append(a)\n",
    "        train_positives.append(p)\n",
    "\n",
    "if SAMPLE_FRAC is not None and SAMPLE_FRAC < 1.0:\n",
    "    train_df = pd.DataFrame({\"anchor\": train_anchors, \"positive\": train_positives})\n",
    "    train_df = train_df.sample(frac=SAMPLE_FRAC, random_state=SEED)\n",
    "    train_anchors = train_df[\"anchor\"].tolist()\n",
    "    train_positives = train_df[\"positive\"].tolist()\n",
    "\n",
    "train_dataset = Dataset.from_dict({\"anchor\": train_anchors, \"positive\": train_positives})\n",
    "eval_dataset = Dataset.from_dict({\"anchor\": eval_anchors, \"positive\": eval_positives}) if eval_anchors else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train pairs: 4615\n",
      "Eval pairs: 548\n",
      "Eval orders: 50\n",
      "\n",
      "train_dataset: Dataset({\n",
      "    features: ['anchor', 'positive'],\n",
      "    num_rows: 4615\n",
      "})\n",
      "eval_dataset: Dataset({\n",
      "    features: ['anchor', 'positive'],\n",
      "    num_rows: 548\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "print(\"Train pairs:\", len(train_anchors))\n",
    "print(\"Eval pairs:\", len(eval_anchors))\n",
    "print(\"Eval orders:\", len(eval_order_ids))\n",
    "print(\"\\ntrain_dataset:\", train_dataset)\n",
    "if eval_dataset is not None:\n",
    "    print(\"eval_dataset:\", eval_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Eval Information retrieval artifacts\n",
    "\n",
    "`eval_queries`, `eval_corpus`, `eval_relevant_docs` for InformationRetrievalEvaluator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_queries = {str(oid): order_id_to_context[oid] for oid in eval_order_ids if oid in order_id_to_context}\n",
    "eval_relevant_docs = {str(oid): set() for oid in eval_order_ids}\n",
    "train_op = pd.read_csv(DATA_DIR / \"order_products__train.csv\")\n",
    "for _, row in train_op.iterrows():\n",
    "    oid = int(row[\"order_id\"])\n",
    "    oid_str = str(oid)\n",
    "    if oid_str in eval_relevant_docs:\n",
    "        eval_relevant_docs[oid_str].add(str(int(row[\"product_id\"])))\n",
    "eval_corpus = {str(pid): text for pid, text in product_text_map.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eval_queries: 50\n",
      "eval_corpus (products): 49688\n",
      "eval_relevant_docs: 50\n",
      "\n",
      "Sample query (first qid):\n",
      "  qid=3105153.0\n",
      "  query: Previously ordered: [weekday 5, hour 8.0] Peppermint Mocha Liquid Coffee Creamer, Chicken & Apple Smoked Chicken Sausage, Chunky Medium Salsa, Organic ...\n",
      "  relevant_docs: []\n"
     ]
    }
   ],
   "source": [
    "print(\"eval_queries:\", len(eval_queries))\n",
    "print(\"eval_corpus (products):\", len(eval_corpus))\n",
    "print(\"eval_relevant_docs:\", len(eval_relevant_docs))\n",
    "print(\"\\nSample query (first qid):\")\n",
    "qid = list(eval_queries.keys())[0]\n",
    "print(f\"  qid={qid}\")\n",
    "print(\"  query:\", eval_queries[qid][:150], \"...\")\n",
    "print(\"  relevant_docs:\", list(eval_relevant_docs.get(qid, []))[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. (Optional) Save to disk\n",
    "\n",
    "Uncomment to write processed outputs to `OUTPUT_DIR`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "# train_dataset.save_to_disk(str(OUTPUT_DIR / \"train_dataset\"))\n",
    "# if eval_dataset is not None:\n",
    "#     eval_dataset.save_to_disk(str(OUTPUT_DIR / \"eval_dataset\"))\n",
    "# import json\n",
    "# with open(OUTPUT_DIR / \"eval_queries.json\", \"w\") as f:\n",
    "#     json.dump(eval_queries, f, indent=0)\n",
    "# with open(OUTPUT_DIR / \"eval_corpus.json\", \"w\") as f:\n",
    "#     json.dump(eval_corpus, f, indent=0)\n",
    "# with open(OUTPUT_DIR / \"eval_relevant_docs.json\", \"w\") as f:\n",
    "#     json.dump({k: list(v) for k, v in eval_relevant_docs.items()}, f, indent=0)\n",
    "# print(\"Saved to\", OUTPUT_DIR)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Instacart (uv .venv)",
   "language": "python",
   "name": "instacart-personalization"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
