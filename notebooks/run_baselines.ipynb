{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baselines Notebook\n",
    "\n",
    "This notebook mirrors `src/baselines/run_baselines.py`: it runs the **content-based** (untrained SBERT) and **collaborative filtering** (item-item) baselines on the same eval set and reports the same IR metrics (Accuracy@1, Accuracy@10, Recall@10, MRR@10, NDCG@10, MAP@100) so you can compare with the trained two-tower SBERT model.\n",
    "\n",
    "- **Content-based:** Same base model (e.g. `all-MiniLM-L6-v2`) with no fine-tuning; ranks by cosine similarity between query and product embeddings.\n",
    "- **CF (item-item):** Co-occurrence in same order from `order_products__prior.csv`; scores candidates by sum of co-occurrence with the user's prior basket. Progress bars show loading, co-occurrence build, eval history, and ranking.\n",
    "\n",
    "Adjust `processed_dir` and `data_dir` below if needed. Set `run_content_based` / `run_cf` to `False` to skip a baseline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup: paths and options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "from src.constants import DEFAULT_DATA_DIR, DEFAULT_PROCESSED_DIR\n",
    "from src.utils import resolve_processed_dir\n",
    "\n",
    "# Resolve to param subdir (e.g. processed/p5_mp20_ef0.1) when needed\n",
    "processed_dir, msg = resolve_processed_dir(DEFAULT_PROCESSED_DIR, DEFAULT_PROCESSED_DIR)\n",
    "data_dir = DEFAULT_DATA_DIR\n",
    "\n",
    "# Which baselines to run (set False to skip)\n",
    "run_content_based = True\n",
    "run_cf = True\n",
    "\n",
    "# Content-based: same base model as training, untrained\n",
    "model_name = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "\n",
    "if msg:\n",
    "    print(msg)\n",
    "print(\"Processed dir:\", processed_dir)\n",
    "print(\"Data dir:\", data_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load eval data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.baselines.collaborative_filtering import load_eval_data\n",
    "\n",
    "eval_queries, eval_corpus, eval_relevant_docs = load_eval_data(processed_dir)\n",
    "print(f\"Eval queries: {len(eval_queries)}, corpus size: {len(eval_corpus)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Content-based baseline (untrained SBERT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if run_content_based:\n",
    "    from src.baselines.content_based import ContentBasedBaseline\n",
    "    from src.baselines.metrics import compute_ir_metrics\n",
    "\n",
    "    print(\"Building content-based (untrained SBERT) baseline...\")\n",
    "    cb = ContentBasedBaseline(eval_queries, eval_corpus, model_name=model_name)\n",
    "    cb_rankings = cb.rank_all()\n",
    "    cb_metrics = compute_ir_metrics(cb_rankings, eval_relevant_docs)\n",
    "\n",
    "    print(\"\\n--- Content-based (untrained SBERT) ---\")\n",
    "    print(f\"  Accuracy@1:   {cb_metrics['accuracy_at_1']:.4f}\")\n",
    "    print(f\"  Accuracy@10:  {cb_metrics['accuracy_at_10']:.4f}\")\n",
    "    print(f\"  Recall@10:    {cb_metrics['recall_at_10']:.4f}\")\n",
    "    print(f\"  MRR@10:       {cb_metrics['mrr_at_10']:.4f}\")\n",
    "    print(f\"  NDCG@10:      {cb_metrics['ndcg_at_10']:.4f}\")\n",
    "    print(f\"  MAP@100:      {cb_metrics['map_at_100']:.4f}\")\n",
    "else:\n",
    "    print(\"Skipped content-based baseline (run_content_based=False).\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collaborative filtering baseline (item-item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if run_cf:\n",
    "    from src.baselines.collaborative_filtering import ItemItemCFBaseline\n",
    "    from src.baselines.metrics import compute_ir_metrics\n",
    "\n",
    "    print(\"Building collaborative filtering (item-item) baseline...\")\n",
    "    cf = ItemItemCFBaseline(data_dir, processed_dir)\n",
    "    cf_rankings = cf.rank_all(eval_query_ids=list(eval_queries.keys()))\n",
    "    cf_metrics = compute_ir_metrics(cf_rankings, eval_relevant_docs)\n",
    "\n",
    "    print(\"\\n--- Collaborative filtering (item-item) ---\")\n",
    "    print(f\"  Accuracy@1:   {cf_metrics['accuracy_at_1']:.4f}\")\n",
    "    print(f\"  Accuracy@10: {cf_metrics['accuracy_at_10']:.4f}\")\n",
    "    print(f\"  Recall@10:    {cf_metrics['recall_at_10']:.4f}\")\n",
    "    print(f\"  MRR@10:       {cf_metrics['mrr_at_10']:.4f}\")\n",
    "    print(f\"  NDCG@10:      {cf_metrics['ndcg_at_10']:.4f}\")\n",
    "    print(f\"  MAP@100:      {cf_metrics['map_at_100']:.4f}\")\n",
    "else:\n",
    "    print(\"Skipped CF baseline (run_cf=False).\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare with SBERT\n",
    "\n",
    "After 4â€“5 epochs the trained two-tower SBERT typically reaches **Accuracy@10 ~0.54**, **Recall@10 ~0.13**, **NDCG@10 ~0.15**. See the README for the full metrics table."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
